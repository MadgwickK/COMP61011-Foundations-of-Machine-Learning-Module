{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxfef0viAGPw"
   },
   "source": [
    "# **Perceptrons** \n",
    "\n",
    "Lab designed by **Professor Christopher Yau** University of Manchester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfIL3dIMBbt0"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This practical is designed to illustrate the construction of a perceptron algorithm for a binary classification task. In this exercise you will:\n",
    "\n",
    "1. Use Python and libraries including numpy (numerical computations) and matplotlib (visualisaton).\n",
    "2. Learn to simulate an artificial dataset to use as test data for the classifer.\n",
    "3. Create functions in Python to perform the individuals steps of the Perceptron algorithm.\n",
    "4. Write a complete perceptron algorithm using all the functions in [3].\n",
    "5. Visualise and plot the outputs.\n",
    "\n",
    "## Pre-requisities\n",
    "\n",
    "This lab exercise assumes some basic programming familiarity with Python. The exercise is designed to avoid excessive use of custom libraries and tries to use as much of standard Python as possible. The exemplar code therefore is designed for readability and interpretability (rather than efficiency) to help you to learn about how the Perceptron works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxM_zI9BBIxg"
   },
   "source": [
    "## Getting Started\n",
    "First, we will import some libraries that will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6ESCRbOKHoz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for visualisation\n",
    "import random # for random number generation\n",
    "import numpy as np # for numerical libraries\n",
    "\n",
    "random.seed(242785) # seed the random number generators\n",
    "np.random.seed(64254)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23OWPey2GO-o"
   },
   "source": [
    "## Simulating data\n",
    "\n",
    "In this part of the code, we will simulate some data using the generative model that underlies the perceptron algorithm.\n",
    "\n",
    "First, we will specify some parameters $\\bf{w}$ for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3NDqbXbFX7F"
   },
   "outputs": [],
   "source": [
    "w0 = 0 # parameter values used to generate data\n",
    "w1 = -1.5\n",
    "w2 = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyEQKz21GrAY"
   },
   "source": [
    "Normally, in an experiment or classification tasks, the input or predictors would be given. But here, we need to generate some input/predictor values $(x_1, x_2)$. We will do this by sampling random vectors from a multivariate normal distribution (https://en.wikipedia.org/wiki/Multivariate_normal_distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnVFtlKUJ-qx"
   },
   "outputs": [],
   "source": [
    "n = 500 # number of training samples to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4-5MiNIKREU"
   },
   "outputs": [],
   "source": [
    "mean = [0, 0] # mean\n",
    "cov = [[3, 0.5], [0.5, 3]] # covariance matrix\n",
    "x1, x2 = np.random.multivariate_normal(mean, cov, n).T # sample from a multivariate normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sko-0kmwHCbF"
   },
   "source": [
    "Now recall we next generate latent variables $z$ using our inputs and parameters and convert into outputs $y$ by taking the sign:\n",
    "$$\n",
    "  z = w_1 x_1 + w_2 x_2 + w_0\n",
    "$$\n",
    "$$\n",
    "  y = sign(z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9id_gc5FiUt"
   },
   "outputs": [],
   "source": [
    "z = w1*x1 + w2*x2 + w0 # generate the latent variable z\n",
    "y = np.sign(z) # generate the output y based on the sign of z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKR07lFTB-Y4"
   },
   "source": [
    "Lets plot the data to see how the two classes separate (or not) in $(x_1, x_2)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "NpW-inqYFkxU",
    "outputId": "8ad05636-3457-4901-9396-8ef364472238"
   },
   "outputs": [],
   "source": [
    "cdict = {1: 'red', 2: 'blue', 3: 'green'} # colour scheme\n",
    "fig, ax = plt.subplots()\n",
    "j = 1\n",
    "for g in np.unique(y):\n",
    "    ix = np.where(y == g)\n",
    "    ax.scatter(x1[ix], x2[ix], c = cdict[j], label = \"y = \" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n",
    "ax.legend()\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d7P4iyiCFhb"
   },
   "source": [
    "The classes do separate so now lets apply our perceptron algorithm and hope it does too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOOci-K6bEUQ"
   },
   "source": [
    "## Perceptron algorithm\n",
    "\n",
    "We will now construct the perceptron algorithm to classify this data. First we will define some useful functions. \n",
    "\n",
    "The first is a prediction function that takes inputs $(x_1, x_2)$ and parameters $(w_0, w_1, w_2)$ and outputs a prediction $\\hat{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMH4Wddk8yRu"
   },
   "outputs": [],
   "source": [
    "# define prediction function\n",
    "def predict_fn(x1, x2, w0, w1, w2):\n",
    "  y_hat = np.sign(w1*x1 + w2*x2 + w0)\n",
    "  return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOq233i09Cb7"
   },
   "source": [
    "Next, we define the hinge loss function which involves predicting for all out input samples using some parameters and then counting how many are different from the observed output:\n",
    "$$\n",
    "  L_{w}(y,\\hat{y}) = -\\sum_{i=1}^n y_i \\hat{y}_i \\mathbb{I}(y_i \\neq \\hat{y}_i)\n",
    "$$\n",
    "where\n",
    "$\n",
    "  \\hat{y}_i = (w_0 + w_1 x_{1,i} + w_2 x_{2,i})\n",
    "$ is the output prediction using the current set of parameters $w$ and $\\mathbb{I}(\\cdot)$ is an indicator function which is 1 if the condition inside the bracket is true and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olShvLFq9HoL"
   },
   "outputs": [],
   "source": [
    "# definition of loss function\n",
    "def loss_fn(x1, x2, y, w0, w1, w2):\n",
    "  y_hat = predict_fn(x1, x2, w0, w1, w2)\n",
    "  loss = -np.sum( y*y_hat*(y != y_hat) )\n",
    "  return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVs8QLfR-eYf"
   },
   "source": [
    "Now we define the stochastic gradient descent (SGD) update function, if $(y_i = 1, \\hat{y} = -1)$ then we update using:\n",
    "\n",
    "\\begin{align}\n",
    "  w_0^{(t+1)} &= w_0^{(t)} + 1, ~ w_1^{(t+1)} = w_1^{(t)} + x_{1,i}, ~ w_2^{(t+1)} = w_2^{(t)} + x_{2,i},\n",
    "\\end{align}\n",
    "\n",
    "else we use\n",
    "\n",
    "\\begin{align}\n",
    "  w_0^{(t+1)} = w_0^{(t)} - 1, ~ w_1^{(t+1)} = w_1^{(t)} - x_{1,i}, ~ w_2^{(t+1)} = w_2^{(t)} - x_{2,i},\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRbh3Kzgbfhp"
   },
   "outputs": [],
   "source": [
    "# define update function\n",
    "def update_fn(x1, x2, y, y_hat, w0, w1, w2):\n",
    "  if ( ( y_hat == -1 ) & ( y == 1 ) ) :\n",
    "    w1 = w1 + x1\n",
    "    w2 = w2 + x2\n",
    "    w0 = w0 + 1\n",
    "  if ( ( y_hat == 1 ) & ( y == -1 ) ) :\n",
    "    w1 = w1 - x1\n",
    "    w2 = w2 - x2\n",
    "    w0 = w0 - 1  \n",
    "  return w0, w1, w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOlJN1Sn_mao"
   },
   "source": [
    "We will now specify how many training epochs to use (an epoch is essentially one update step) and the initial parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR4I7lXSLYUG"
   },
   "outputs": [],
   "source": [
    "n_epochs = 5000 # number of training epochs\n",
    "w1_hat = -0.5 # initialise parameter estimate\n",
    "w2_hat = 1.5\n",
    "w0_hat = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xgb6r3MbDf6"
   },
   "source": [
    "Now we begin the perceptron algorithm but first we set up some arrays to hold the parameter and loss values as they change with each update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrrD_5YQ8qx1"
   },
   "outputs": [],
   "source": [
    "loss_vec = np.zeros(n_epochs)\n",
    "w0_vec = np.zeros(n_epochs)\n",
    "w1_vec = np.zeros(n_epochs)\n",
    "w2_vec = np.zeros(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPG00b_gCJgP"
   },
   "source": [
    "Now lets iterate the perceptron update steps up to the maximum number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coaVj3QYLZAf"
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs): # for each epoch\n",
    "  \n",
    "  # sample one of the training data points\n",
    "  i = random.randint(0, n-1)\n",
    "  y_i = y[i]\n",
    "  x1_i = x1[i]\n",
    "  x2_i = x2[i]\n",
    "  \n",
    "  # predict the output using the current parameters and input\n",
    "  y_hat_i = predict_fn(x1_i, x2_i, w0_hat, w1_hat, w2_hat) \n",
    "  \n",
    "  # update the parameters\n",
    "  w0_hat, w1_hat, w2_hat = update_fn(x1_i, x2_i, y_i, y_hat_i, w0_hat, w1_hat, w2_hat)\n",
    "  \n",
    "  # compute the new loss \n",
    "  loss = loss_fn(x1, x2, y, w0_hat, w1_hat, w2_hat)\n",
    "\n",
    "  # store\n",
    "  loss_vec[epoch] = loss\n",
    "  w0_vec[epoch] = w0_hat\n",
    "  w1_vec[epoch] = w1_hat\n",
    "  w2_vec[epoch] = w2_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jk60_-ZCYz6"
   },
   "source": [
    "After the perceptron has finished running, we can plot the loss function to check for convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "dODVnWNWPkwF",
    "outputId": "c48bf038-4dbe-482a-d75a-d64fa3fe94fc"
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(0, n_epochs), loss_vec, s=16)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygthMkC-FcdL"
   },
   "source": [
    "Lets see the classification using the true and estimated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbxnDGt0NPdW"
   },
   "outputs": [],
   "source": [
    "# compute the predictions for all the training data\n",
    "y_hat = predict_fn(x1, x2, w0_hat, w1_hat, w2_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "oiv13kMzsjSh",
    "outputId": "2c81c6f2-5dca-43b1-b5cf-b00badc0d929"
   },
   "outputs": [],
   "source": [
    "# set up the plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "j = 1\n",
    "for g in np.unique(y):\n",
    "    ix = np.where(y == g)\n",
    "    ax1.scatter(x1[ix], x2[ix], c = cdict[j], label = \"y = \" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n",
    "\n",
    "j = 1\n",
    "for g in np.unique(y_hat):    \n",
    "    ix = np.where(y_hat == g)\n",
    "    ax2.scatter(x1[ix], x2[ix], c = cdict[j], label = \"$\\hat{y} = $\" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKErx1IOTyau"
   },
   "source": [
    "Let compute the decision boundaries, recall from the lecture that at the decision boundary:\n",
    "$$\n",
    "  z = 0 = w_1 x_1 + w_2 x_2 + w_0\n",
    "$$\n",
    "or \n",
    "$$\n",
    "  x_2 = -\\frac{1}{w_2}(w_1 x_1 + w_0 )\n",
    "$$\n",
    "lets compute this both for the true parameters and the estimated ones from the perceptron to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqZAMQJtT3mK"
   },
   "outputs": [],
   "source": [
    "# compute decision boundary direction\n",
    "x1_start = -2.5\n",
    "x1_end = 2.5\n",
    "x2_start = -(w0 + w1*x1_start)/w2\n",
    "x2_end = -(w0 + w1*x1_end)/w2\n",
    "x2_start_est = -(w0_hat + w1_hat*x1_start)/w2_hat\n",
    "x2_end_est = -(w0_hat + w1_hat*x1_end)/w2_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dh3cqE8UR3R"
   },
   "source": [
    "Now, lets add these decision boundaries to the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "NY1RaUlmT1a4",
    "outputId": "5c52862f-bfc7-4492-8f2a-380a9cb95c91"
   },
   "outputs": [],
   "source": [
    "# set up the plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "j = 1\n",
    "for g in np.unique(y):\n",
    "    ix = np.where(y == g)\n",
    "    ax1.scatter(x1[ix], x2[ix], c = cdict[j], label = \"y = \" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n",
    "\n",
    "j = 1\n",
    "for g in np.unique(y_hat):    \n",
    "    ix = np.where(y_hat == g)\n",
    "    ax2.scatter(x1[ix], x2[ix], c = cdict[j], label = \"$\\hat{y} = $\" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n",
    "\n",
    "# plot the decision boundaries\n",
    "ax1.plot([x1_start, x1_end], [x2_start, x2_end], 'k-')\n",
    "ax2.plot([x1_start, x1_end], [x2_start_est, x2_end_est], 'k--')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax1.set_xlim(-3, 3) \n",
    "ax2.set_xlim(-3, 3) \n",
    "ax1.set_ylim(-4, 4) \n",
    "ax2.set_ylim(-4, 4) \n",
    "ax1.set_xlabel(\"$x_1$\")\n",
    "ax1.set_ylabel(\"$x_2$\")\n",
    "ax2.set_xlabel(\"$x_1$\")\n",
    "ax1.set_title('Ground Truth')\n",
    "ax2.set_title('Perceptron')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAPYNfW3FT2y"
   },
   "source": [
    "The two plots look near-identical so lets highlight the classification errors to make it easier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "zYDpqnEXFQdY",
    "outputId": "e2d56152-5b3c-435c-c0d4-1c94e71ef070"
   },
   "outputs": [],
   "source": [
    "classification_error = (y!=y_hat) # compute which training samples are misclassified\n",
    "fig2, ax = plt.subplots()\n",
    "j = 1\n",
    "for g in np.unique(classification_error):\n",
    "    ix = np.where(classification_error == g)\n",
    "    ax.scatter(x1[ix], x2[ix], c = cdict[j], label = \"Error: \" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n",
    "ax.legend()\n",
    "ax.plot([x1_start, x1_end], [x2_start, x2_end], 'k-')\n",
    "ax.plot([x1_start, x1_end], [x2_start_est, x2_end_est], 'k--')\n",
    "ax.set_xlim(-3, 3) \n",
    "ax.set_ylim(-4, 4) \n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DdJ-qzOM7V8"
   },
   "source": [
    "If the data can be entirely linearly separated, there will be no classification errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJZ97QgFBrX-"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Modify the data simulation procedure to:\n",
    "\n",
    "1. Increase/decrease the separation between the two classes, what happens to the performance of the classifier?\n",
    "2. Vary the number of training samples, how does this alter the behaviour of the algorithm?\n",
    "3. Can you plot a graph of degree of separation/number of training samples versus classifier performance?\n",
    "\n",
    "How happens if you use a data generating procedure which is not the one assumed by the perceptron algorithm?\n",
    "\n",
    "## Extended Exercises [Optional]\n",
    "\n",
    "The following exercises are not compulsory but would test your understanding and should be done in your own time (they will not be covered in the labs): \n",
    "\n",
    "1.   Re-design the code to allow for any number of inputs (you will need to use matrix/vector operations provided by the numpy library).\n",
    "2.   Read about the \"Perceptron Convergence Theorem\" (https://en.wikipedia.org/wiki/Perceptron#Convergence), show empirically that the performance of your code is consistent with the theorem.\n",
    "3.   Apply your code to data from the UCI Data Repository (https://archive.ics.uci.edu/ml/datasets.php)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "perceptron-lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
