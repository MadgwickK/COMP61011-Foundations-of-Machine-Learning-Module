{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6hyrQ2Ag6bs"
   },
   "source": [
    "# **Logistic Regression** \n",
    "\n",
    "Lab designed by **Professor Christopher Yau** University of Manchester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nsem8tvHg_fw"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This practical is designed to illustrate the construction of a Logistic Regression model for a binary classification task. In this exercise you will:\n",
    "\n",
    "1. Use Python and libraries including numpy (numerical computations) and matplotlib (visualisaton).\n",
    "2. Learn to simulate an artificial dataset to use as test data for the classifer.\n",
    "3. Create functions in Python to perform the individuals steps of a Logistic Regression model.\n",
    "4. Write a complete Logistic Regression based classifier using all the functions in [3].\n",
    "5. Visualise and plot the outputs.\n",
    "\n",
    "## Pre-requisities\n",
    "\n",
    "This lab exercise assumes some basic programming familiarity with Python. The exercise is designed to avoid excessive use of custom libraries and tries to use as much of standard Python as possible. The exemplar code therefore is designed for readability and interpretability (rather than efficiency) to help you to learn about how Logistic Regression works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq18g828g_Ys"
   },
   "source": [
    "## Getting Started\n",
    "First, we will import some libraries that will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNm4HKPwg3_U"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for visualisation\n",
    "import random # for random number generation\n",
    "import numpy as np # for numerical libraries\n",
    "\n",
    "random.seed(242785) # seed the random number generators\n",
    "np.random.seed(64254)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg3AxCv9hVYZ"
   },
   "source": [
    "## Simulating data\n",
    "\n",
    "In this part of the code, we will simulate some data using the generative model that underlies the logistic regression model.\n",
    "\n",
    "First, we will specify some parameters $\\bf{w}$ for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8wrCaX3hatV"
   },
   "outputs": [],
   "source": [
    "w0 = 0 # parameter values used to generate data\n",
    "w1 = -1.5\n",
    "w2 = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oqd6awshe_2"
   },
   "source": [
    "Normally, in an experiment or classification tasks, the input or predictors would be given. But here, we need to generate some input/predictor values $(x_1, x_2)$. We will do this by sampling random vectors from a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fp6dDF7-hhza"
   },
   "outputs": [],
   "source": [
    "n = 50 # number of training samples to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGKwri0FhjbK"
   },
   "outputs": [],
   "source": [
    "mean = [0, 0] # mean\n",
    "cov = [[3, 0.5], [0.5, 3]] # covariance matrix\n",
    "x1, x2 = np.random.multivariate_normal(mean, cov, n).T # sample from a multivariate normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8QxiGBchlpv"
   },
   "source": [
    "Now recall we next generate latent variables $z$ of the form using our inputs and parameters and convert into outputs $y$ by taking the sign:\n",
    "$$\n",
    "  z = w_1 x_1 + w_2 x_2 + w_0,\n",
    "$$\n",
    "$$\n",
    "  h = \\frac{1}{1+\\exp(-z)},\n",
    "$$\n",
    "$$\n",
    "  p(y=1) = h, \n",
    "  p(y=0) = 1-h.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhLShTuvh6Q0"
   },
   "outputs": [],
   "source": [
    "z = w1*x1 + w2*x2 + w0\n",
    "h = 1/(1+np.exp(-z))\n",
    "y = np.zeros(n)\n",
    "for i in range(n):\n",
    "  y[i] = np.random.choice(np.array([0, 1]), 1, replace=True, p=[1-h[i], h[i]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJcLNqonjvOl"
   },
   "source": [
    "Lets plot the data to see how the two classes separate (or not) in $(x_1, x_2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U1_DYBDfju1J"
   },
   "outputs": [],
   "source": [
    "cdict = {1: 'red', 2: 'blue', 3: 'green'} # colour scheme\n",
    "fig, ax = plt.subplots()\n",
    "j = 1\n",
    "for g in np.unique(y):\n",
    "    ix = np.where(y == g)\n",
    "    ax.scatter(x1[ix], x2[ix], c = cdict[j], label = \"y = \" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n",
    "ax.legend()\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbg-4a5RkLpk"
   },
   "source": [
    "The classes do separate so now lets apply our logistic regression algorithm does this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwWtW9BkkSNQ"
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We will now construct the logistic regression classifier to classify this data. First we will define some useful functions. \n",
    "\n",
    "The first is a prediction function that takes inputs $(x_1, x_2)$ and parameters $(w_0, w_1, w_2)$ and outputs a prediction $\\hat{y}$:\n",
    "$$\n",
    "\th = p(y = 1|{\\bf x},{\\bf w}) = \\frac{1}{1 + \\exp(-\\underbrace{[{\\bf w}^t {\\bf x} + w_0]}_z) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aigRKN95kZUK"
   },
   "outputs": [],
   "source": [
    "# define the logistic function\n",
    "def logistic_fn(z):\n",
    "  h = 1/(1+np.exp(-z))\n",
    "  return h\n",
    "  \n",
    "# define prediction function\n",
    "def predict_fn(x1, x2, w0, w1, w2):\n",
    "  z = w1*x1 + w2*x2 + w0\n",
    "  h = logistic_fn(z)\n",
    "  return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ih491yQbtsoI"
   },
   "outputs": [],
   "source": [
    "# define classification function\n",
    "def classify_fn(d, x1, x2, w0, w1, w2):\n",
    "  z = w1*x1 + w2*x2 + w0\n",
    "  h = logistic_fn(z)\n",
    "  ix = np.where(h < d)\n",
    "  y = np.zeros(x1.size)\n",
    "  y[ix] = 1.0\n",
    "  return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLuCMGB_ws2y"
   },
   "source": [
    "The loss function is given by:\n",
    "\\begin{align*}\n",
    "\tl({\\bf w}) = - \\frac{1}{n} \\log L({\\bf w}|D) = - \\frac{1}{n} \\sum_{i=1}^n \\left [ y_i \\log h({\\bf w}, {\\bf x}_i) + (1-y_i) \\log ( 1 - h({\\bf w}, {\\bf x}_i) \\right ]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi6oVU28k4_N"
   },
   "outputs": [],
   "source": [
    "# definition of loss function\n",
    "def loss_fn(x1, x2, y, w0, w1, w2):\n",
    "  h = predict_fn(x1, x2, w0, w1, w2)\n",
    "  loss = -np.sum( y*np.log(h) + (1-y)*np.log(1-h) )\n",
    "  return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJiBauQJwbJ-"
   },
   "source": [
    "The general form of a SGD update is to take the current value of the parameter and then to subtract off the gradient scaled by a learning rate in this case denoted by $\\lambda$:\n",
    "  $$\n",
    "\t\tw_j \\leftarrow w_j  + \\lambda [ y_i - h({\\bf w}, {\\bf x}_i) ] x_i^j \n",
    "\t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Fpb4c2tpmBZ"
   },
   "outputs": [],
   "source": [
    "# definition of update function\n",
    "def update_fn(y_i, h_i, x1_i, x2_i, w0, w1, w2, n, batch_size, learning_rate):\n",
    "  w1 = w1 + (n/batch_size)*learning_rate*np.sum((y_i - h_i)*x1_i)\n",
    "  w2 = w2 + (n/batch_size)*learning_rate*np.sum((y_i - h_i)*x2_i)\n",
    "  w0 = w0 + (n/batch_size)*learning_rate*np.sum((y_i - h_i))  \n",
    "  return w0, w1, w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um8qdBpjvm_w"
   },
   "source": [
    "We will now specify how many training epochs to use (an epoch is essentially one update step), learning rate and mini-batch sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4jJMlaWlUdD"
   },
   "outputs": [],
   "source": [
    "n_epochs = 5000 # number of training epochs\n",
    "learning_rate = 0.001 # learning rate\n",
    "batch_size = 1 # mini-batch siz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgTOsknbvsqf"
   },
   "source": [
    "Initialise the parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YY0yPNmxvkLd"
   },
   "outputs": [],
   "source": [
    "w1_hat = 0 # initialise parameter estimate\n",
    "w2_hat = 0\n",
    "w0_hat = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OliruGDUvgcX"
   },
   "source": [
    "Now we begin the logistic regression classification but first we set up some arrays to hold the parameter and loss values as they change with each update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBevkJehlWik"
   },
   "outputs": [],
   "source": [
    "loss_vec = np.zeros(n_epochs)\n",
    "w0_vec = np.zeros(n_epochs)\n",
    "w1_vec = np.zeros(n_epochs)\n",
    "w2_vec = np.zeros(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoA-ngiyveOj"
   },
   "source": [
    "Now lets iterate the perceptron update steps up to the maximum number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiMsPrDilY8X"
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs): # for each epoch\n",
    "  \n",
    "  # sample a mini-batch of training data points\n",
    "  ix = np.random.choice(n, batch_size, replace=False) \n",
    "  y_i = y[ix]\n",
    "  x1_i = x1[ix]\n",
    "  x2_i = x2[ix]\n",
    "  \n",
    "  # predict\n",
    "  h_i = predict_fn(x1_i, x2_i, w0_hat, w1_hat, w2_hat)\n",
    "\n",
    "  # update \n",
    "  w0_hat, w1_hat, w2_hat = update_fn(y_i, h_i, x1_i, x2_i, w0_hat, w1_hat, w2_hat, n, batch_size, learning_rate)\n",
    "\n",
    "  # compute loss\n",
    "  loss = loss_fn(x1, x2, y, w0_hat, w1_hat, w2_hat)\n",
    "\n",
    "  # store\n",
    "  loss_vec[epoch] = loss\n",
    "  w0_vec[epoch] = w0_hat\n",
    "  w1_vec[epoch] = w1_hat\n",
    "  w2_vec[epoch] = w2_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZn1REYyvR6D"
   },
   "source": [
    "After the perceptron has finished running, we can plot the loss function to check for convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3IlYJt1oXbP"
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(0, n_epochs), loss_vec, s=16)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G--xmT9evMxc"
   },
   "source": [
    "Lets see the classification using the true and estimated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHZQ85D2rBBf"
   },
   "outputs": [],
   "source": [
    "d = 0.5 # decision threshold\n",
    "y_hat = classify_fn(d, x1, x2, w0_hat, w1_hat, w2_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTiFfRz0qaWe"
   },
   "outputs": [],
   "source": [
    "# set up the plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "j = 1\n",
    "for g in np.unique(y):\n",
    "    ix = np.where(y == g)\n",
    "    ax1.scatter(x1[ix], x2[ix], c = cdict[j], label = \"y = \" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n",
    "\n",
    "j = 1\n",
    "for g in np.unique(y_hat):    \n",
    "    ix = np.where(y_hat == g)\n",
    "    ax2.scatter(x1[ix], x2[ix], c = cdict[j], label = \"$\\hat{y} = $\" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqtOZ1Azu369"
   },
   "source": [
    "Let compute the decision boundaries, recall from the lecture that at the decision boundary:\n",
    "$$\n",
    "  z = \\log \\left ( \\frac{d}{1-d} \\right ) = w_1 x_1 + w_2 x_2 + w_0\n",
    "$$\n",
    "or \n",
    "$$\n",
    "  x_2 = \\frac{1}{w_2} \\left ( \\log \\left ( \\frac{d}{1-d} \\right ) - w_1 x_1 - w_0 \\right )\n",
    "$$\n",
    "lets compute this both for the true parameters and the estimated ones from the perceptron to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "muAvKjags96s"
   },
   "outputs": [],
   "source": [
    "# compute decision boundary direction\n",
    "x1_start = -2.5\n",
    "x1_end = 2.5\n",
    "x2_start = (np.log(d/(1-d)) - w0 - w1*x1_start)/w2\n",
    "x2_end = (np.log(d/(1-d)) - w0 - w1*x1_end)/w2\n",
    "x2_start_est = (np.log(d/(1-d)) - w0_hat - w1_hat*x1_start)/w2_hat\n",
    "x2_end_est = ( np.log(d/(1-d)) - w0_hat - w1_hat*x1_end)/w2_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aR7Z6mXvs9CI"
   },
   "outputs": [],
   "source": [
    "# set up the plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "j = 1\n",
    "for g in np.unique(y):\n",
    "    ix = np.where(y == g)\n",
    "    ax1.scatter(x1[ix], x2[ix], c = cdict[j], label = \"y = \" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n",
    "\n",
    "j = 1\n",
    "for g in np.unique(y_hat):    \n",
    "    ix = np.where(y_hat == g)\n",
    "    ax2.scatter(x1[ix], x2[ix], c = cdict[j], label = \"$\\hat{y} = $\" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n",
    "\n",
    "# plot the decision boundaries\n",
    "ax1.plot([x1_start, x1_end], [x2_start, x2_end], 'k-')\n",
    "ax2.plot([x1_start, x1_end], [x2_start_est, x2_end_est], 'k--')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax1.set_xlim(-3, 3) \n",
    "ax2.set_xlim(-3, 3) \n",
    "ax1.set_ylim(-4, 4) \n",
    "ax2.set_ylim(-4, 4) \n",
    "ax1.set_xlabel(\"$x_1$\")\n",
    "ax1.set_ylabel(\"$x_2$\")\n",
    "ax2.set_xlabel(\"$x_1$\")\n",
    "ax1.set_title('Ground Truth')\n",
    "ax2.set_title('Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VlyQOyNv6zg"
   },
   "source": [
    "The two plots look near-identical so lets highlight the classification errors to make it easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSzhK8k7s87w"
   },
   "outputs": [],
   "source": [
    "classification_error = (y!=y_hat) # compute which training samples are misclassified\n",
    "fig2, ax = plt.subplots()\n",
    "j = 1\n",
    "for g in np.unique(classification_error):\n",
    "    ix = np.where(classification_error == g)\n",
    "    ax.scatter(x1[ix], x2[ix], c = cdict[j], label = \"Error: \" + str(g), s = 16, alpha=0.3)\n",
    "    j = j + 1\n",
    "ax.legend()\n",
    "ax.plot([x1_start, x1_end], [x2_start, x2_end], 'k-')\n",
    "ax.plot([x1_start, x1_end], [x2_start_est, x2_end_est], 'k--')\n",
    "ax.set_xlim(-3, 3) \n",
    "ax.set_ylim(-4, 4) \n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6b7OJQTwCiU"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "Modify the data simulation procedure to:\n",
    "\n",
    "1. Increase/decrease the separation between the two classes, what happens to the performance of the classifier?\n",
    "2. Vary the number of training samples, how does this alter the behaviour of the algorithm?\n",
    "3. Can you plot a graph of degree of separation/number of training samples versus classifier performance?\n",
    "\n",
    "How happens if you use a data generating procedure which is not the one assumed by the logistic regression algorithm?\n",
    "\n",
    "## Extended Exercises [Optional]\n",
    "\n",
    "The following exercises are not compulsory but would test your understanding and should be done in your own time (they will not be covered in the labs): \n",
    "\n",
    "1.   Re-design the code to allow for any number of inputs (you will need to use matrix/vector operations provided by the numpy library).\n",
    "2.   Apply your code to data from the UCI Data Repository (https://archive.ics.uci.edu/ml/datasets.php)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LogisticRegression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
