{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnYpopk92TRB"
   },
   "source": [
    "# Optimisation\n",
    "\n",
    "With thanks to Prof. Christopher Yau, University of Oxford.\n",
    "\n",
    "## Overview\n",
    "\n",
    "This practical is designed to illustrate the construction and use of numerical optimisation techniques. In this exercise you will:\n",
    "\n",
    "1.   Understand the use of objective functions and how they are derived for simple problems,\n",
    "2.   Implement a gradient descent algorithm,\n",
    "3.   Implement a stochastic gradient descent algorithm.\n",
    "\n",
    "## Pre-requisities\n",
    "\n",
    "This lab exercise assumes some basic programming familiarity with Python. The exercise is designed to avoid excessive use of custom libraries and tries to use as much of standard Python as possible. The exemplar code therefore is designed for readability and interpretability (rather than efficiency) to help you to learn about how the different optimisation techniques work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MhDmGyZ3Pxh"
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "First, we will import some libraries that will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0tWt0yW3Kvy"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # for visualisation\n",
    "import random # for random number generation\n",
    "import numpy as np # for numerical libraries\n",
    "\n",
    "random.seed(242785) # seed the random number generators\n",
    "np.random.seed(64254) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr3vAby0BX1B"
   },
   "source": [
    "### Synthesising data\n",
    "\n",
    "We will need to some data to work with. For training and model development purposes, it is often convenient to *simulate* data. This  allows you full control over how the data is generated and to later understand if your model works under perfect conditions.\n",
    "\n",
    "For this exercise, we are going to simulate data for a simple linear regression model. Here we have a input variable $X$ and an output variable $Y$. \n",
    "\n",
    "The relationship between $X$ and $Y$ is given by:\n",
    "$$\n",
    "  Y = a X + b\n",
    "$$\n",
    "where $a$ and $b$ are model parameters (also known as *regression coefficients* in this context).\n",
    "\n",
    "In order to simulate data, we therefore have to specify values for $a$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx_8h6doCjWB"
   },
   "outputs": [],
   "source": [
    "a_true = 2\n",
    "b_true = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DERymkoRCl92"
   },
   "source": [
    "For good coding practice, we shall add the subscript `_true` to the variable names to distinguish these values as the true values used to generate the data as supposed to the *inferred* values we will estimate later from data.\n",
    "\n",
    "Next, we will need some $X$ values. Lets generate $n=30$ of these values uniformly spaced between 0 and 1 using the `linspace` function from the numpy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mw35zMj7DF5Y"
   },
   "outputs": [],
   "source": [
    "n = 30\n",
    "x = np.linspace(0, 1, num=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDFIhh5vDV04"
   },
   "source": [
    "Now, we need to calculate the $Y$-values according to our given relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CniPyw9GDbQG"
   },
   "outputs": [],
   "source": [
    "y_true = a_true*x + b_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoO2fWEGDon6"
   },
   "source": [
    "Lets plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "0rwQY8rPDmJt",
    "outputId": "44735232-7087-4e16-ba49-c0a3d7fa8fb1"
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y_true, 'r')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xccTAPz8DqiK"
   },
   "source": [
    "You should see a straight line with slope given by $a$ and intercept on the vertical axis by $b$.\n",
    "\n",
    "Now, this data is so far not very realistic, there is no noise that is normally associated with real-world measurements. We can add the noise by simulating random variables from a Normal distribution. Here, we can use the numpy random number generator (`np.random.normal`): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGivfk7A3k1H"
   },
   "outputs": [],
   "source": [
    "sigma2_true = 0.2 # noise variance\n",
    "e = np.random.normal(loc=0.0, scale=np.sqrt(sigma2_true), size=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XybsF_68EdIH"
   },
   "source": [
    "A normal distribution has two parameters, the mean (`loc`) of the distribution and its standard deviation (`scale`). Instead of using standard deviation, we specify the variance of the noise instead (which is the standard deviation squared) so we have to square-root it before using it as input into `np.random.normal`. \n",
    "\n",
    "Random number generators in different software libraries sometimes use variance and sometimes they use standard deviation.\n",
    "\n",
    "Now, lets add the noise we have generated to the true $Y$-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLPnGKipERsb"
   },
   "outputs": [],
   "source": [
    "y = y_true + e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPsa_J1RFkT3"
   },
   "source": [
    "Now lets plot the noisy $(X, Y)$ values over the true ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "6aDzaJVw4Z4J",
    "outputId": "75c2154f-d1c7-48ec-a69f-51b2fa8006de"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s=16)\n",
    "plt.plot(x, y_true, 'r')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCfrwL6FFrQG"
   },
   "source": [
    "Now we have simulated noisy data for a system where there is a underlying true linear relationship between $X$ and $Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9nJy3XxelcT"
   },
   "source": [
    "## Learning\n",
    "\n",
    "We now have (simulated) data contained in the variables $(X, Y)$. In a real-world setting, we might plot this data to try to understand feasible relationships between the model (which we have done already above).\n",
    "\n",
    "Given this visualisation we might hypothesis that the relationship $Y = aX + b$ applies (in fact, we know this is the actual true model but we wouldn't for a real problem). \n",
    "\n",
    "Now, the **inference problem** is to identify the parameters $(a, b)$ that make our model most consistent with the data.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "To identify the best data-fitting parameters, we need to define a **loss function** which tells us, for any given set of parameters, how well the model fits the data. What is a suitable loss function?\n",
    "\n",
    "Given a dataset $D = \\{ (x_1, y_1), \\dots, (x_n, y_n) \\}$, one approach is to construct a loss function based on the error between the real measured value of $Y$ and the one predicted by the model $\\hat{Y}$ based on parameters $\\theta = \\{a,b\\}$.\n",
    "\n",
    "The loss function sums over all the errors across the $n$ data points:\n",
    "$$\n",
    "  L(\\theta) = \\sum_{i=1}^n ( Y_i - \\hat{Y}_i )^2\n",
    "$$\n",
    "where the prediction is given by $\\hat{Y_i} = aX_i+b$. \n",
    "\n",
    "We square each error term to ensure that each data point contributes a strictly positive-value to the total loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MR5A5z8AxW91"
   },
   "outputs": [],
   "source": [
    "def loss_fn(a, b, x, y):\n",
    "  y_hat = a*x + b # generate prediction of Y based on X and parameters (a, b)\n",
    "  L = np.sum( (y- y_hat)**2 ) # compute total loss\n",
    "  return L # return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDA4LW8Scaz0"
   },
   "source": [
    "### Optimisation\n",
    "\n",
    "The goal is to find parameters that *minimise* the total loss:\n",
    "$$\n",
    "  \\hat{\\theta} = \\arg \\min_{\\theta} L(\\theta)\n",
    "$$\n",
    "\n",
    "We will use **gradient descent** to find the parameters that minimise the loss function:\n",
    "\n",
    "$$\n",
    "  \\theta' \\leftarrow \\theta - \\lambda \\nabla L(\\theta)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDqotWug4zVS"
   },
   "outputs": [],
   "source": [
    "n_its = 30 # number of optimisation steps\n",
    "lam = 0.01 # step size\n",
    "\n",
    "a = 1 # initial value for a\n",
    "b = -1 # initial value for b\n",
    "\n",
    "# set up some arrays to store output from each step of the optimisation process\n",
    "loss_values = np.zeros(n_its)\n",
    "a_values = np.zeros(n_its)\n",
    "b_values = np.zeros(n_its)\n",
    "\n",
    "# iterate \n",
    "for it in range(n_its):\n",
    "\n",
    "  y_hat = a*x + b # get predicted values for y based on the current parameter values for (a,b)\n",
    "  \n",
    "  dlda = np.sum( -2*(y-y_hat)*(x) ) # compute the derivative of the loss function with respect to a, dL/da\n",
    "  dldb = np.sum( -2*(y-y_hat)*(1) ) # compute the derivative of the loss function with respect to b, dL/db\n",
    "\n",
    "  a = a - lam*dlda # update a via gradient descent\n",
    "  b = b - lam*dldb # update b via gradient descent\n",
    "\n",
    "  loss_values[it] = loss_fn(a, b, x, y) # compute loss function at current parameters\n",
    "  a_values[it] = a # store current a value\n",
    "  b_values[it] = b # store current b value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEXS4A_igAqN"
   },
   "source": [
    "### Assessing convergence\n",
    "\n",
    "It is good practice to plot the loss values after you have run the optimisation procedure. This is to check that the algorithm has truly *converged* to the minimum of the loss function. Further, the values of the parameters being estimated should also converge. This can be helpful for identifying bugs in code too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "4UfQD7w1eztE",
    "outputId": "3fb0a636-dc8f-437d-efab-87e322e0aaca"
   },
   "outputs": [],
   "source": [
    "# set up a multiple plot figure (3 rows, 1 column) with three plotting axes\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 12))\n",
    "\n",
    "# plot the loss values\n",
    "ax1.plot(loss_values, 'o-')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "\n",
    "# plot the a values\n",
    "ax2.plot(a_values, 'o-')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('a')\n",
    "\n",
    "# plot the b values\n",
    "ax3.plot(b_values, 'o-')\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32LrEERTySlt"
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Now lets repeat the exercise with stochastic gradient descent (SGD).\n",
    "\n",
    "With SGD instead of computing the entire gradient term which requires evaluating all $n$ data points, we choose a small batch of data points and use this subset to estimate the gradient. For the special case when the batch size is one, the procedure is sometimes known as **online SGD** because only one data point need be held in computer memory at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQBQxUQtgJ-h"
   },
   "outputs": [],
   "source": [
    "n_its = 3000\n",
    "lam = 0.01\n",
    "a = 1\n",
    "b = -1\n",
    "\n",
    "loss_values = np.zeros(n_its)\n",
    "a_values = np.zeros(n_its)\n",
    "b_values = np.zeros(n_its)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q41smprYya8c"
   },
   "source": [
    "The main difference is that at each step of our optimisation procedure we now randomly sample a data pair $(X_j, Y_j)$ which we use to estimate the gradient rather than all $n$ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zknvJMgyZwc",
    "outputId": "1fc33cc7-f7be-4b7f-86dc-2536b50cb3f7"
   },
   "outputs": [],
   "source": [
    "for it in range(n_its):\n",
    "    \n",
    "    # sample one random data pair (x, y)\n",
    "    j = np.random.randint(0, n-1)\n",
    "    xj = x[j]\n",
    "    yj = y[j]\n",
    "    yj_hat = a*xj + b \n",
    "    \n",
    "    dlda = np.sum( -2*(yj-yj_hat)*(xj) )\n",
    "    dldb = np.sum( -2*(yj-yj_hat)*(1) )\n",
    "    a = a - lam*n*dlda # multiply by n to get estimate of gradient\n",
    "    b = b - lam*n*dldb # multiply by n to get estimate of gradient\n",
    "\n",
    "    loss_values[it] = loss_fn(a, b, x, y)\n",
    "    a_values[it] = a\n",
    "    b_values[it] = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22ZBFgawyz6j"
   },
   "source": [
    "The result is an $n$-fold reduction in compute time per iteration but because the gradients are only estimated the overall procedure is *noisy*. On average, we are minimising the loss but there is some variation.\n",
    "\n",
    "This shows that while SGD benefits from reduced compute and memory requirements since one does not need to compute gradients from all data points, we may need a larger number of optimisation steps to identify the minimum of the loss function and the overall procedure is more unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "hl2OAxkBglTx",
    "outputId": "1e1ca826-6b00-45c8-a599-de51b9f51ddd"
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 12))\n",
    "#fig.suptitle('A tale of 2 subplots')\n",
    "\n",
    "ax1.plot(loss_values, 'o-')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "\n",
    "ax2.plot(a_values, 'o-')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('a')\n",
    "\n",
    "ax3.plot(b_values, 'o-')\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPT_izIbjlfY"
   },
   "source": [
    "Study the effect of changing the total number of optimisation steps and the size of the step size $\\lambda$. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsiM8m7ywlZQ"
   },
   "source": [
    "## Assignment Exercise\n",
    "\n",
    "By editing the code above, consider the following: \n",
    "\n",
    "1. What is the impact of changing the total number of optimisation steps?\n",
    "2. What is the impact of changing the step size, $\\lambda$?\n",
    "3. We have provided code for standard gradient descent and online (batch = 1) stochastic gradient descent. What is the impact of changing the batchsize?\n",
    "4. Try changing the value of a_true and b_true. Does this have any impact on any of the parameters you have considered in Q1-3?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "optimisation_Qs_only.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
